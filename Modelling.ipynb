{"cells":[{"cell_type":"markdown","metadata":{"id":"g_h3-HEASrMb"},"source":["### Modeling\n","\n","With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."]},{"cell_type":"markdown","source":["Linear Regression\n","No hyperparameters\n","I'm going to use it as a base line"],"metadata":{"id":"W5jiMprIVNaZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqfmYREiSrMb","executionInfo":{"status":"ok","timestamp":1738732207115,"user_tz":480,"elapsed":315,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"923b4b5b-b7a9-4338-a317-ee0724ab4ccf"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-2c31a22ab4fd>:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  X_train[feature].fillna(X_train[feature].median(), inplace=True)\n","<ipython-input-15-2c31a22ab4fd>:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  X_test[feature].fillna(X_train[feature].median(), inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Model 1 using features: ['year', 'odometer_log', 'age', 'price_per_mile']\n","  Mean Squared Error: 129011559.82\n","  Mean Absolute Error: 8729.75\n","\n","Model 2 using features: ['year', 'odometer_log', 'age', 'price_per_mile', 'condition_ordinal', 'title_status_ordinal', 'size_ordinal']\n","  Mean Squared Error: 127873911.88\n","  Mean Absolute Error: 8658.76\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.impute import SimpleImputer\n","\n","# Model 1: Simple model\n","model1_features = ['year', 'odometer_log', 'age', 'price_per_mile']\n","\n","#Create SimpleImputer instance: Created an instance of SimpleImputer with strategy='median'.\n","# This will replace NaN values with the median of the respective column, I could use main as well\n","imputer = SimpleImputer(strategy='median')\n","\n","# Model 2: Extended model with the ordinal encoded columns.\n","model2_features = model1_features + ['condition_ordinal', 'title_status_ordinal', 'size_ordinal']\n","\n","# Model 3: Full model using all features (all columns in X_train)\n","model3_features = X_train.columns.tolist()\n","\n","# --- Build, fit, and evaluate the models ---\n","\n","MyResults = {}  # to store evaluation metrics for each model\n","\n","# Model 1\n","model1 = LinearRegression()\n","\n","# Impute NaN values in X_train and X_test with the median (or another strategy) before fitting\n","# For each column in model1_features, fill NaN with the median of that column in X_train\n","for feature in model1_features:\n","    X_train[feature].fillna(X_train[feature].median(), inplace=True)\n","    X_test[feature].fillna(X_train[feature].median(), inplace=True)\n","\n","model1.fit(X_train[model1_features], y_train)\n","pred1 = model1.predict(X_test[model1_features])\n","mse1 = mean_squared_error(y_test, pred1)\n","mae1 = mean_absolute_error(y_test, pred1)\n","MyResults['Model 1'] = {'Features': model1_features, 'MSE': mse1, 'MAE': mae1}\n","\n","# Model 2\n","model2 = LinearRegression()\n","# use the imputer\n","X_train_model2 = pd.DataFrame(imputer.fit_transform(X_train[model2_features]), columns=model2_features, index=X_train.index)\n","X_test_model2 = pd.DataFrame(imputer.transform(X_test[model2_features]), columns=model2_features, index=X_test.index)\n","\n","model2.fit(X_train_model2, y_train)\n","pred2 = model2.predict(X_test_model2)\n","\n","mse2 = mean_squared_error(y_test, pred2)\n","mae2 = mean_absolute_error(y_test, pred2)\n","MyResults['Model 2'] = {'Features': model2_features, 'MSE': mse2, 'MAE': mae2}\n","\n","# Model 3  -------------------------------------------------------------------------------------\n","# the full linear regression give me errors ValueError: Cannot use median strategy with non-numeric data: could not convert string to float:\n","# need more time to debug\n","# model3 = LinearRegression()\n","#I got this error The error \"ValueError: Cannot use median strategy with non-numeric data: could not convert string to float:\n","#I have to identify numerocal features in  X_train[model3_features] using select dtypes(include=np.number) saving in numerical_features_model\n","# Select only numerical features for imputation\n","# numerical_features_model3 = X_train[model3_features].select_dtypes(include=np.number).columns.tolist()\n","\n","# Apply imputation only to numerical features\n","# X_train_model3_num = pd.DataFrame(imputer.fit_transform(X_train[numerical_features_model3]),\n","#                                  columns=numerical_features_model3, index=X_train.index)\n","#X_test_model3_num = pd.DataFrame(imputer.transform(X_test[numerical_features_model3]),\n","#                                 columns=numerical_features_model3, index=X_test.index)\n","\n","# X_train_model3 = pd.concat([X_train_model3_num, X_train[model3_features].select_dtypes(exclude=np.number)], axis=1)\n","# X_test_model3 = pd.concat([X_test_model3_num, X_test[model3_features].select_dtypes(exclude=np.number)], axis=1)\n","# Previous code with error : X_train_model3 = pd.DataFrame(imputer.fit_transform(X_train[model3_features]), columns=model3_features, index=X_train.index)\n","# Previous code with error : X_test_model3 = pd.DataFrame(imputer.transform(X_test[model3_features]), columns=model3_features, index=X_test.index)\n","# model3.fit(X_train_model3, y_train)\n","# pred3 = model3.predict(X_test_model3)\n","# mse3 = mean_squared_error(y_test, pred3)\n","# mae3 = mean_absolute_error(y_test, pred3)\n","# MyResults['Model 3'] = {'Features': model3_features, 'MSE': mse3, 'MAE': mae3}\n","\n","# --- Print the evaluation metrics for each model ---\n","for model_name, res in MyResults.items():\n","    print(f\"\\n{model_name} using features: {res['Features']}\")\n","    print(f\"  Mean Squared Error: {res['MSE']:.2f}\")\n","    print(f\"  Mean Absolute Error: {res['MAE']:.2f}\")\n","\n","### --->>> NEED TO APPLY SEQUENTIAL FEATURE SELECTION w9.2 w9.3 w9.4 Rdge Model <<<< -------------\n","\n","### ->>. GridSearchCV Best Alphaiterating over alphas\n"]},{"cell_type":"markdown","source":["2 Ridge Regression\n","Parameter = alpha  but the range should be in log scale 10^-3 to 10^3  "],"metadata":{"id":"lVKvHyoBVjm5"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.stats import uniform\n","\n","features_Ridge = ['year', 'odometer_log', 'age', 'price_per_mile']\n","# Option 1 using GridSearchCV to tune it\n","# Define Ridge estimator with a random state equal 42 to repro acn compare\n","# plan to tuse auto and svd as the algorithms to solve the best coef,  I believe we\n","# have some correlation between variables in the dataset.\n","MyRidge = Ridge(random_state=42)\n","MyGridSearchCV_params = {\n","    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver':['auto', 'svd'],\n","    'max_iter':[None,100,1000]\n","}\n","# Set up GridSearchCV with 5-fold cross-validation and negative MSE as the scoring metric\n","grid_search = GridSearchCV(\n","    estimator= MyRidge,\n","    param_grid= MyGridSearchCV_params,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1\n",")\n","grid_search.fit(X_train[features_Ridge], y_train)\n","\n","# Retrieve the best Ridge estimator found by GridSearchCV\n","best_ridge_grid = grid_search.best_estimator_\n","print(\"Best parameters from GridSearchCV:\", grid_search.best_params_)\n","\n","# Evaluate the best estimator on the test set\n","prediction_grid = best_ridge_grid.predict(X_test[features_Ridge])\n","\n","mse_grid = mean_squared_error(y_test, prediction_grid)\n","mae_grid = mean_absolute_error(y_test, prediction_grid)\n","print(\"GridSearchCV Ridge - Test MSE:\", mse_grid)\n","print(\"GridSearchCV Ridge - Test MAE:\", mae_grid)\n","\n","\n","# Using RandomizeSearchCV to compare with GridSearchCV\n","# let's see :-)  using the same params\n","GridSearchCV_params = {\n","    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver':['auto', 'svd'],\n","    'max_iter':[None,100,1000]\n","}\n","MyRandom_search = RandomizedSearchCV(\n","    estimator= MyRidge,\n","    param_distributions= GridSearchCV_params,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1,\n","    n_iter=10,\n","    random_state=42\n",")\n","\n","MyRandom_search.fit(X_train[features_Ridge], y_train)\n","best_ridge_random = MyRandom_search.best_estimator_\n","print(\"Best parameters from RandomizedSearchCV:\", MyRandom_search.best_params_)\n","\n","prediction_MyRandom_search = best_ridge_random.predict(X_test[features_Ridge])\n","print(\"MyRandomizedSearchCV Ridge -Test MSE:\",mean_squared_error(y_test, prediction_MyRandom_search))\n","print(\"MyRandomizedSearchCV Ridge -Test MAE:\",mean_absolute_error(y_test, prediction_MyRandom_search))\n","\n","\n","\n"],"metadata":{"id":"lOKAStmKVhBq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739842973521,"user_tz":480,"elapsed":9307,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"483f64a3-a952-48c8-a98b-b81c3711204e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters from GridSearchCV: {'alpha': 1, 'max_iter': None, 'solver': 'auto'}\n","GridSearchCV Ridge - Test MSE: 4.464198509715481e-06\n","GridSearchCV Ridge - Test MAE: 0.0016075500395790945\n","Best parameters from RandomizedSearchCV: {'solver': 'svd', 'max_iter': 100, 'alpha': 1}\n","MyRandomizedSearchCV Ridge -Test MSE: 4.4641985097154785e-06\n","MyRandomizedSearchCV Ridge -Test MAE: 0.0016075500395791073\n"]}]},{"cell_type":"markdown","source":["3 Lasson Regression"],"metadata":{"id":"yEsDeiNkWP7E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEuSYLp4SrMc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843969305,"user_tz":480,"elapsed":15436,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"c39cc352-fe80-4f0c-813d-92e506a866a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters from GridSearchCV (Lasso): {'alpha': 0.001, 'max_iter': 1000, 'selection': 'cyclic', 'tol': 0.0001}\n","GridSearchCV Lasso - Test MSE: 5.7222877858253145e-06\n","GridSearchCV Lasso - Test MAE: 0.001990068505051068\n","Best parameters from RandomizedSearchCV (Lasso): {'alpha': 37.455011884736244, 'max_iter': 1000, 'selection': 'cyclic', 'tol': 0.01}\n","RandomizedSearchCV Lasso - Test MSE: 5.7222877858253145e-06\n","RandomizedSearchCV Lasso - Test MAE: 0.001990068505051068\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import Lasso\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.stats import uniform\n","\n","# Define the feature set (using the same features as for Ridge)\n","MyFeatures_Lasso = ['year', 'odometer_log', 'age', 'price_per_mile']\n","\n","# ---------------------------\n","# Option 1: Using GridSearchCV for Lasso\n","\n","# Initialize Lasso estimator with a fixed random_state for reproducibility\n","my_lasso = Lasso(random_state=42)\n","\n","# Define the parameter grid for GridSearchCV.\n","# Note: Lasso does not have a 'solver' parameter like Ridge, but it offers 'selection' (cyclic or random)\n","param_grid_lasso = {\n","    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'max_iter': [1000, 5000],\n","    'tol': [0.0001, 0.001, 0.01],\n","    'selection': ['cyclic', 'random']\n","}\n","\n","# Set up GridSearchCV with 5-fold cross-validation\n","lasso_grid_search = GridSearchCV(\n","    estimator=my_lasso,\n","    param_grid=param_grid_lasso,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1\n",")\n","\n","# Fit GridSearchCV on the training data\n","lasso_grid_search.fit(X_train[MyFeatures_Lasso], y_train)\n","\n","# Retrieve the best Lasso estimator found\n","best_lasso_grid = lasso_grid_search.best_estimator_\n","print(\"Best parameters from GridSearchCV (Lasso):\", lasso_grid_search.best_params_)\n","\n","# Evaluate the best estimator on the test set\n","pred_lasso_grid = best_lasso_grid.predict(X_test[MyFeatures_Lasso])\n","mse_lasso_grid = mean_squared_error(y_test, pred_lasso_grid)\n","mae_lasso_grid = mean_absolute_error(y_test, pred_lasso_grid)\n","print(\"GridSearchCV Lasso - Test MSE:\", mse_lasso_grid)\n","print(\"GridSearchCV Lasso - Test MAE:\", mae_lasso_grid)\n","\n","# ---------------------------\n","# Option 2: Using RandomizedSearchCV for Lasso\n","\n","# Define parameter distributions for RandomizedSearchCV.\n","# For 'alpha', we use a uniform distribution.  Let's see how it works .. :-)\n","param_dist_lasso = {\n","    'alpha': uniform(0.001, 100),\n","    'max_iter': [1000, 5000],\n","    'tol': [0.0001, 0.001, 0.01],\n","    'selection': ['cyclic', 'random']\n","}\n","\n","lasso_random_search = RandomizedSearchCV(\n","    estimator=my_lasso,\n","    param_distributions=param_dist_lasso,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1,\n","    n_iter=10,  # reduce iterations to speed up the search\n","    random_state=42\n",")\n","\n","# Fit RandomizedSearchCV on the training data\n","lasso_random_search.fit(X_train[MyFeatures_Lasso], y_train)\n","\n","# Retrieve the best Lasso estimator found\n","best_lasso_random = lasso_random_search.best_estimator_\n","print(\"Best parameters from RandomizedSearchCV (Lasso):\", lasso_random_search.best_params_)\n","\n","# Evaluate the best estimator on the test set\n","pred_lasso_random = best_lasso_random.predict(X_test[MyFeatures_Lasso])\n","mse_lasso_random = mean_squared_error(y_test, pred_lasso_random)\n","mae_lasso_random = mean_absolute_error(y_test, pred_lasso_random)\n","print(\"RandomizedSearchCV Lasso - Test MSE:\", mse_lasso_random)\n","print(\"RandomizedSearchCV Lasso - Test MAE:\", mae_lasso_random)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"oMK8jvWdyjXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NbQkWRSSrMc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SK1pgtrSrMc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"woONt4XjSrMc"},"source":["### Evaluation\n","\n","With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."]},{"cell_type":"markdown","source":["#### Evaluation Answer 1\n","\n","Both models perform very similar, but the Ridge model shows a slight advantage based on error metrics:\n","\n","Ridge Regression:\n","Test MSE: ~4.464e-06\n","Test MAE: ~0.001608\n","Lasso Regression:\n","Test MSE: ~5.722e-06\n","Test MAE: ~0.001990\n","\n"],"metadata":{"id":"av_CoXI2zoLt"}},{"cell_type":"markdown","source":["#### Evaluation Answer 2####\n","\n","1. Model Performance: The Ridge model achieved lower MSE and MAE values compared to Lasso. Maybe Ridge might be better at capturing the underlying relationships without introducing too much bias with a acceptable speed\n","\n","2. Regularization Differences: Ridge Regression applies L2 regularization which tends to shrink coefficients but rarely zeroes them out. Lasso Regression applies L1 regularization, However, If most of the  features are useful, the sparsity induced by Lasso may not be as beneficial and could even hurt performance slightly. But at this stage I'm not sure because I haven't had the time to do a more complete analysis. I think that is something I should do.\n","\n","3. Hyperparameter:\n","Both GridSearchCV and RandomizedSearchCV converged to similar performance metrics within each model, suggesting that the hyperparameter search was good.\n","For Ridge, both searches pointed to alpha = 1\n","For Lasso, although the best parameters differ between GridSearchCV and RandomizedSearchCV, the performance remained the same, indicating that several hyperparameter combinations may yield similar outcomes.\n","\n","4. Choosing the Best Model\n","Given the slightly lower error metrics, Ridge Regression looks like the best for this tasks. However, since the differences are relatively small, it’s also worth considering other factors and the potential need for more feature selection."],"metadata":{"id":"R3PtUYb4z6LY"}},{"cell_type":"markdown","source":["#### Evaluation Answer 3 ####\n","\n","What earlier adjustments can be done in the Data Preparation could be adjusted to get better results\n","\n","1. Refine Outlier Detection: Play more with IQR Factor: Instead of using a fixed factor (e.g., 1.5) for all variables, I could consider adjusting it per variable based on their distributions.\n","\n","2. Enhanced Missing Value Imputation: Iterative Imputation or Separate Separate Imputation Strategies: Tailor imputation methods for different features. For instance, I coud use mode imputation for categorical features and mean/median (or even a predictive model) for numerical features.\n","\n","3. Feature Transformation and Scaling: Target and Feature Transformation:\n","Apply logarithmic transformations to more variables (e.g., price) to stabilize variance and make the distributions more normal I only use log transformation to odometer\n","\n","4. More Correlation Analysis.  With more time I could examine multicollinearidad between predictors to remove or combine them in new features\n","\n"],"metadata":{"id":"XP1kGldL2USK"}},{"cell_type":"markdown","metadata":{"id":"Ww1JQJqeSrMc"},"source":["### Deployment\n","\n","Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."]},{"cell_type":"markdown","source":["### Used Car Sales Optimization : A Data-Driven Approach###\n","\n","**Introduction**\n","\n","In today’s competitive market, understanding what drives used car prices is crucial to managing your inventory effectively. My recent analysis leverages data science techniques to help you optimize your vehicle selection and pricing strategy. By identifying the key factors that influence car values, you can make better purchasing decisions, adjust pricing appropriately, and ultimately improve your profit margins.\n","\n","**What I analyzed**\n","\n","I examined a large dataset containing detailed information on hundreds of thousands of used (and new) cars. Key variables in my analysis included:\n","\n","Year: The production year of the vehicle, which helps determine its age.\n","Mileage (Odometer): Total miles driven, indicating wear and tear.\n","Price per Mile: A derived metric that divides the car’s price by its mileage, offering insight into relative value.\n","Other Factors: Additional features such as the vehicle's condition and manufacturer were also considered during the analysis.\n","\n","**The Modeling Process**\n","\n","To predict used car prices accurately, I applied two types of regression models:\n","Ridge Regression: This model applies a technique called L2 regularization, which shrinks the impact of each variable in a balanced way. My results showed that Ridge Regression performed very well, suggesting that all features in your dataset contribute valuable information to the final price.\n","Lasso Regression: Lasso uses L1 regularization, which can effectively “turn off” less important features. However, my analysis found that while Lasso was also effective, Ridge Regression slightly outperformed it, indicating that none of the features were completely redundant in predicting car prices.\n","\n","**Key Findings from the Models**\n","\n","**Ridge Regression Results:**\n","\n","Mean Squared Error (MSE): 4.46e-06\n","Mean Absolute Error (MAE): 0.00161\n","\n","**Lasso Regression Results:**\n","MSE: 5.72e-06\n","MAE: 0.00199\n","\n","The lower error rates in Ridge Regression mean that, on average, its predictions are closer to the actual sale prices. This suggests that when all factors are considered, each one plays a role in determining the car’s value.\n","\n","\n","**How This Helps You Fine-Tune your Inventory**\n","\n","1. Accurate Pricing Based on Key Features\n","Vehicle Age: Newer models tend to hold their value better. Use the car’s production year and calculated age to adjust pricing.\n","Mileage and Price per Mile: Lower mileage often indicates a higher value per mile. By comparing similar vehicles, you can identify bargains or overpriced listings.\n","2. Data-Driven Purchasing Decisions\n","Focus on Well-Rounded Vehicles: Since Ridge Regression shows that all measured features contribute to the final price, consider a holistic approach when evaluating potential purchases. Vehicles that score well across multiple factors are likely to be strong investments.\n","3. Inventory and Pricing Adjustments\n","Dynamic Pricing: Use these insights to set competitive prices that reflect the true market value, reducing the time vehicles spend on your lot.\n","Targeted Inventory Acquisition: Knowing which factors have the greatest impact on price can help you identify underpriced vehicles with potential for profit after refurbishment or market correction.\n","4. Continuous Improvement\n","Refining Data Collection: The more accurate and detailed your data (e.g., precise mileage, detailed condition reports), the more reliable your pricing models will be.\n","Regular Reassessment: The market is always evolving. Regularly updating your models with fresh data ensures that your inventory strategies remain aligned with current market trends.\n","Conclusion\n","Using data science, you can gain a competitive edge in the used car market. This analysis shows that a comprehensive approach—taking into account vehicle age, mileage, and other key features—provides a more accurate picture of a car’s value. In my study, Ridge Regression emerged as the more reliable tool for predicting prices, which suggests that a balanced consideration of all available features is beneficial.\n","\n","###My Recommendations###\n","\n","####Long term####\n","\n","Adopt data-driven pricing: Adjust your pricing strategy based on the factors that truly impact a car’s value.\n","Enhance your data collection: Gather detailed and accurate information on each vehicle.\n","Continuously review and update models: As market conditions change, so should your models and strategies.\n","\n","\n","\n","####Short Term####\n","\n","1. **Daily in Store  Inventory Assessment**  Morning Routine:\n","Run the Model on Current Inventory:\n","What to Do: Every morning, update your inventory data with the latest details (e.g., production year, mileage, condition).\n","How to Do It: Use the Python script you’ve built to feed your current inventory through the Ridge model. This model will output a predicted price for each vehicle.\n","Compare Predictions to Listed Prices:\n","What to Look For: Create a report that shows both the predicted price and the current listing price.\n","How to Act:\n","Overpriced Vehicles: If a car’s listing price is significantly above the predicted price, consider lowering the price or running a special promotion.\n","Underpriced Vehicles: If the listing price is lower than the predicted price, you might have room to increase the price—or if it’s selling too quickly, consider buying more of that model.\n","\n","2. **Pricing Adjustments**\n","In-Store and Online:\n","Adjust Listing Prices:\n","Action Step: Based on the morning report, update your online listings and in-store price tags.\n","Tip: Create a pricing band (e.g., within ±5% of the predicted price) as a target range for consistency.\n","Promotional Strategies:\n","Action Step: For vehicles that are overpriced compared to the model, consider running flash sales or discounts to move inventory faster.\n","Tip: Use the model’s confidence in its predictions as a guide—if the prediction error is low, you can be more aggressive with pricing changes.\n","\n","3. **Inventory Optimization**\n","Ongoing Inventory Review:\n","Identify High-Potential Vehicles:\n","Action Step: Use your model to flag vehicles that show a high “value per mile” or have lower mileage with a good production year.\n","How to Act: Prioritize these vehicles in marketing efforts or consider expanding your stock of similar models.\n","Evaluate Underperformers:\n","Action Step: For vehicles with a low predicted price relative to market demand, plan targeted promotions (e.g., “quick sale” discounts).\n","Tip: Keep a record of these vehicles to review if repeated adjustments are needed or if they should be phased out in future orders.\n","\n","4. **Weekly/Monthly  Analysis and Continuous Improvement**\n","End-of-Week and End-of-Month Review:\n","Analyze Sales Performance:\n","Action Step: At the end of each week, compare the actual sale prices with the model’s predictions.\n","How to Use: Identify trends—are vehicles sold closer to the predicted price? Are there any particular changes leading to faster sales? Enrich the model with those features\n","Adjust Procurement Decisions:\n","Action Step: Based on weekly/monthly performance, refine your buying strategy:\n","Increase orders for high-performing models.\n","Reduce or discount models that consistently underperform.\n","Feedback Loop:\n","Action Step: Update your models with the latest sales data. This helps improve prediction accuracy over time.\n","5. **Training and Maintenance**\n","Keep Your Team Trained and Informed:\n","Ongoing Training:\n","Action Step: Schedule periodic training sessions on how to interpret the model outputs and dashboard metrics.\n","How to Act: Share best practices and recent insights from your weekly reviews to empower your team in making data-driven decisions.\n","Tool Updates:\n","Action Step: Maintain and update your data pipeline. Ensure that new inventory data is incorporated in real time and that your models are re-trained as market conditions evolve\n","Keep adding new features as you discover they could affect the sales.\n","Call Data scientist to update the model  \n"],"metadata":{"id":"r9Xg3aSOPOrR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AggDm79KSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yT-qzQSGSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqBl5E5dSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NE1OSUn9SrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn_IOiyWSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TZZ9JcVSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBY9vTbBSrMd"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[{"file_id":"1jCcBlOvQYDxvdIqFUWPpu1qI4scapAhX","timestamp":1740031642318},{"file_id":"1L5jQkraVEGX4Rqg8-CidH6lDCk6PXetX","timestamp":1738636979539}]}},"nbformat":4,"nbformat_minor":0}