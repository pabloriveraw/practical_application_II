{"cells":[{"cell_type":"markdown","metadata":{"id":"Geyk-SqWSrMU"},"source":["# What drives the price of a car?\n","\n","![](images/kurt.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"adhUSzbOSrMX"},"source":["**OVERVIEW**\n","\n","In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."]},{"cell_type":"markdown","metadata":{"id":"WKAqzU3ZSrMY"},"source":["### CRISP-DM Framework\n","\n","<center>\n","    <img src = images/crisp.png width = 50%/>\n","</center>\n","\n","\n","To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."]},{"cell_type":"markdown","metadata":{"id":"Q4x1Z5TDSrMY"},"source":["### Business Understanding\n","\n","From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary."]},{"cell_type":"markdown","source":["**Business Understanding - Answer1**\n","\n","The business task \"Identify key drivers for used cars prices\" can be framed as a data problem assuming that the target variable is the 'car price' then I can use supervised regression to predict the used card price. This can be achieved based on features ( or predictor variables ) like the year, odometer, menaufacturer, etc."],"metadata":{"id":"B93J0KW8XgIX"}},{"cell_type":"markdown","source":["**Business Understanding - Answer2**:\n","\n","The Plan for using the CRISP-DM framework is after to get a solid enough business understanding,  have to transition to the Data Understanding Phase to run exploratory data analysis to check data quality such as completeness, confiability, and consistency across the data set. The next phase: Data Prep I will clean data and run feature engineering in order to prepare the data set for the next phases. Once I have the data prepared I can move to the Modeling phase where I can develop and validate predictive models—such as linear regression or ensemble methods—to quantify the influence of each predictor on the price, using metrics like RMSE to assess model performance. Finally, The model will predict prices but also is going to identify and rank the key drivers that impact on used car pricing to provide actionable insights that can resolve the original business problem.\n"],"metadata":{"id":"PFioENhZkzCh"}},{"cell_type":"markdown","metadata":{"id":"LDy1xwWVSrMa"},"source":["### Data Understanding\n","\n","After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."]},{"cell_type":"markdown","source":["**Data Understanding - Answer1**\n","\n","My exploratory activities were:\n","Identify key drivers for used car prices means in terms of data tasks\n","1. Look for data sources of used cars: In this case Kaggle\n","2. Download in my phython environment\n","3. Initial data inspection:  using cars.info() and cars.head() to review the structure, columns names, data types, samples"],"metadata":{"id":"gJg2OvLJkrc2"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSPB_KsTSrMa","executionInfo":{"status":"ok","timestamp":1739936370845,"user_tz":480,"elapsed":23037,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"ccabef33-2af8-4773-9a87-88c4c59f69b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Berkeley/Unit11/practical_application_II')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b7sCVGg-SrMa","executionInfo":{"status":"ok","timestamp":1739936384023,"user_tz":480,"elapsed":9758,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","cars =pd.read_csv('data/vehicles.csv')"]},{"cell_type":"markdown","source":["Metadata Extraction\n","Understand what each attribute represents, noting the type of data (numerical, categorical, etc.) and any available metadata or data dictionaries. using cars.info(). cars.columns\n","Data Quality Assessment"],"metadata":{"id":"UfmI_MRHq35O"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQyaHjVtSrMb","executionInfo":{"status":"ok","timestamp":1739936388322,"user_tz":480,"elapsed":737,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"ea8e5443-7d42-4e67-b2e4-4f0827123f87"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 426880 entries, 0 to 426879\n","Data columns (total 18 columns):\n"," #   Column        Non-Null Count   Dtype  \n","---  ------        --------------   -----  \n"," 0   id            426880 non-null  int64  \n"," 1   region        426880 non-null  object \n"," 2   price         426880 non-null  int64  \n"," 3   year          425675 non-null  float64\n"," 4   manufacturer  409234 non-null  object \n"," 5   model         421603 non-null  object \n"," 6   condition     252776 non-null  object \n"," 7   cylinders     249202 non-null  object \n"," 8   fuel          423867 non-null  object \n"," 9   odometer      422480 non-null  float64\n"," 10  title_status  418638 non-null  object \n"," 11  transmission  424324 non-null  object \n"," 12  VIN           265838 non-null  object \n"," 13  drive         296313 non-null  object \n"," 14  size          120519 non-null  object \n"," 15  type          334022 non-null  object \n"," 16  paint_color   296677 non-null  object \n"," 17  state         426880 non-null  object \n","dtypes: float64(2), int64(2), object(14)\n","memory usage: 58.6+ MB\n","None\n","Index(['id', 'region', 'price', 'year', 'manufacturer', 'model', 'condition',\n","       'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'VIN',\n","       'drive', 'size', 'type', 'paint_color', 'state'],\n","      dtype='object')\n","           id                  region  price  year manufacturer model  \\\n","0  7222695916                prescott   6000   NaN          NaN   NaN   \n","1  7218891961            fayetteville  11900   NaN          NaN   NaN   \n","2  7221797935            florida keys  21000   NaN          NaN   NaN   \n","3  7222270760  worcester / central MA   1500   NaN          NaN   NaN   \n","4  7210384030              greensboro   4900   NaN          NaN   NaN   \n","\n","  condition cylinders fuel  odometer title_status transmission  VIN drive  \\\n","0       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n","1       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n","2       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n","3       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n","4       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n","\n","  size type paint_color state  \n","0  NaN  NaN         NaN    az  \n","1  NaN  NaN         NaN    ar  \n","2  NaN  NaN         NaN    fl  \n","3  NaN  NaN         NaN    ma  \n","4  NaN  NaN         NaN    nc  \n"]}],"source":["print(cars.info())\n","print(cars.columns)\n","print(cars.head())"]},{"cell_type":"markdown","source":["**Data Understanding - Answer2**\n","\n","Data Description & Summary Statistics.\n","\n","1. I looked into summary statistics for numerical columns with df.describe() to understand the central tendencies, dispersion, and range and possibility to do some feature engineering.\n","2. I did exploration of Categorical Data:   cars['manufacturer'].value_counts() to explore the distribution of categorical features\n","3. I reviewed the Quality of the data searching for Missing Values Analysis: Identify missing or null values using methods like df.isnull().sum().\n","4. I looked for outliers in numerical columns. I used some visualization\n"],"metadata":{"id":"YhboKUq5rzqn"}},{"cell_type":"code","source":["print(cars.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBbfFOuCsBF-","executionInfo":{"status":"ok","timestamp":1738545793886,"user_tz":480,"elapsed":208,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"46e49b5c-8ea9-4e71-bc1d-1f8e1fcdfd9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                 id         price           year      odometer\n","count  4.268800e+05  4.268800e+05  425675.000000  4.224800e+05\n","mean   7.311487e+09  7.519903e+04    2011.235191  9.804333e+04\n","std    4.473170e+06  1.218228e+07       9.452120  2.138815e+05\n","min    7.207408e+09  0.000000e+00    1900.000000  0.000000e+00\n","25%    7.308143e+09  5.900000e+03    2008.000000  3.770400e+04\n","50%    7.312621e+09  1.395000e+04    2013.000000  8.554800e+04\n","75%    7.315254e+09  2.648575e+04    2017.000000  1.335425e+05\n","max    7.317101e+09  3.736929e+09    2022.000000  1.000000e+07\n"]}]},{"cell_type":"code","source":["print(cars.isnull().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0voNxjzisvCE","executionInfo":{"status":"ok","timestamp":1738533987140,"user_tz":480,"elapsed":420,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"da029d36-3527-41ef-a755-c4546ea35a0c","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["id                   0\n","region               0\n","price                0\n","year              1205\n","manufacturer     17646\n","model             5277\n","condition       174104\n","cylinders       177678\n","fuel              3013\n","odometer          4400\n","title_status      8242\n","transmission      2556\n","VIN             161042\n","drive           130567\n","size            306361\n","type             92858\n","paint_color     130203\n","state                0\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["Outlier Identification: Outliers in numerical columns with visualizations.\n","Outlier Identification: Outliers in numerical columns using Interquartile Range (IQR) method to identify outliers:   \n","The Interquartile Range (IQR)  IQR = Q3 - Q1\n","Outliers values should fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR\n","Further I can adjust the parameter 1.5 to adjust the sensitivity\n"],"metadata":{"id":"FAh11oJ4p_fj"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","num_cols = ['price', 'year', 'odometer']\n","for col in num_cols:\n","    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","    for col in num_cols:\n","      fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","    # Boxplot for quick visual identification of outliers\n","    sns.boxplot(x=cars[col], ax=axes[0])\n","    axes[0].set_title(f'Boxplot of {col}')\n","\n","    # Histogram with a kernel density estimate for a detailed distribution view\n","    sns.histplot(cars[col], kde=True, ax=axes[1])\n","    axes[1].set_title(f'Histogram of {col}')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    sns.boxplot(x=cars[col], ax=axes[0])\n","    axes[0].set_title(f'Boxplot of {col}')\n","\n","    # Histogram with a kernel density estimate for a detailed distribution view\n","    sns.histplot(cars[col], kde=True, ax=axes[1])\n","    axes[1].set_title(f'Histogram of {col}')\n","\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"t77m8tirtEKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_outliers(df, col, factor=1.5):\n","    \"\"\"\n","    Detect outliers in a column of a DataFrame using the IQR method.\n","\n","    Parameters:\n","        df (pd.DataFrame): The DataFrame containing the data.\n","        col (str): The name of the column to analyze.\n","        factor (float): The multiplier to determine the outlier threshold (default 1.5).\n","\n","    Returns:\n","        outliers (pd.DataFrame): Subset of the DataFrame containing the outlier rows.\n","        lower_bound (float): The lower threshold.\n","        upper_bound (float): The upper threshold.\n","    \"\"\"\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - factor * IQR\n","    upper_bound = Q3 + factor * IQR\n","\n","    # Filter rows that have values below the lower bound or above the upper bound\n","    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n","    return outliers, lower_bound, upper_bound\n","\n","# using the outlier detection function to each numerical column..\n","for col in num_cols:\n","    outliers, lower_bound, upper_bound = detect_outliers(cars, col)\n","    print(f\"Column: {col}\")\n","    print(f\"  Lower bound: {lower_bound}\")\n","    print(f\"  Upper bound: {upper_bound}\")\n","    print(f\"  Number of detected outliers: {outliers.shape[0]}\")\n","    print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AYwaT6qu5-p","executionInfo":{"status":"ok","timestamp":1738549511921,"user_tz":480,"elapsed":184,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"5f5f723d-ac71-4538-9baa-d96c816b415c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Column: price\n","  Lower bound: -24978.625\n","  Upper bound: 57364.375\n","  Number of detected outliers: 8177\n","--------------------------------------------------\n","Column: year\n","  Lower bound: 1994.5\n","  Upper bound: 2030.5\n","  Number of detected outliers: 15896\n","--------------------------------------------------\n","Column: odometer\n","  Lower bound: -106053.75\n","  Upper bound: 277300.25\n","  Number of detected outliers: 4385\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["**Data Understanding - Answer3**\n","\n","Some conclusions:\n","From the odometer boxplot and histogram I can conclude: there are a large quantity of odometer values that can be consider as outliers, which is in line with IQR rule.  The Data looks very skewed to lower values near zero. The Distribution range is highly packed around lower values, but the outliers stretch across the entire range of the x-axis, reaching close to 1.0 (normalized data).This high concentration in low values sugggest possible data quality issues or scale issues. In the next step, I should separate the analysis: Main distribution and other for outliers, to check if there is a real data issue or some error induced by normalization."],"metadata":{"id":"2PNdFAxmRRMx"}},{"cell_type":"markdown","metadata":{"id":"HsCqf-zBSrMb"},"source":["### Data Preparation\n","\n","After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`."]},{"cell_type":"markdown","source":["**Data Preparation - Answer1**\n","\n","**Handling missing values and data consistency**\n","1. Evaluate Missing Values: I Looked into missing values count per column to decide if I input or drop columns/rows based on the percentage of missing values. If A column has too many missing values and isn't critical, I should drop.\n","\n","2. Imputation criteria:  For numerical columns, I will consider using the median for imputation\n","For Categorical columns: I'll use the mode or a constant ( \"not found\")\n","\n"],"metadata":{"id":"gnhsYQO0W02M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hbvYA8jSrMb"},"outputs":[],"source":["import pandas as pd\n","\n","#. cars = pd.read_csv('data/vehicles.csv'). ##  LOST MY PREVIOUS CAR\n","\n","# Check missing values\n","missing_summary = cars.isnull().sum()\n","print(missing_summary)\n","\n","# Impute numerical columns with median (example for 'year' and 'odometer')\n","cars['year'].fillna(cars['year'].median(), inplace=True)\n","cars['odometer'].fillna(cars['odometer'].median(), inplace=True)\n","\n","# Impute categorical columns with a placeholder\n","for col in ['manufacturer', 'model', 'condition', 'cylinders', 'fuel',\n","            'title_status', 'transmission', 'VIN', 'drive', 'size', 'type', 'paint_color']:\n","    cars[col].fillna('Unknown', inplace=True)"]},{"cell_type":"markdown","source":["**Data Preparation - Answer2**\n","\n","**Outlier handling & data Transformation**\n","\n","1. Capping the outliers:  For columns like odometer with extreme high values, I will cap the them at a high percentile ( 99th )  to reduce their influence.\n","\n","2. Log Transformation for Skewed Distributions:\n","Odometer is heavily right-skewed, a log transformation can help normalize the distribution. (Use np.log1p to handle zero values safely.)"],"metadata":{"id":"BecCKBj3aQlq"}},{"cell_type":"code","source":["import numpy as np\n","\n","def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n","    lower_bound = series.quantile(lower_quantile)\n","    upper_bound = series.quantile(upper_quantile)\n","    return series.clip(lower=lower_bound, upper=upper_bound)\n","\n","# Cap outliers for odometer\n","cars['odometer_capped'] = cap_outliers(cars['odometer'])"],"metadata":{"id":"-vpkMw6vaM89"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrOCTJIRSrMb"},"outputs":[],"source":["# Apply log transformation to the original or capped odometer column\n","cars['odometer_log'] = np.log1p(cars['odometer_capped'])"]},{"cell_type":"markdown","source":["**Data Preparation - Answer3**\n","\n","**Feature Engineering**\n","\n","1. ***Car Age*** : Instead of using the raw \"year\". I will compute the car's age relative to the current year (or the year of data collection). This is more useful because the Depreciation of the car is measure in how old thus is easier to compare  15-year-old and 20-year-old vehicles, respectively to understand how the depreciation could affect the price. Also, this approach could reduce the risk of incorporate time specific trends that could be present in the original raw data.  ***Age*** suggest me a better feature to find correlations because its refers to how long the car has been used. It can correlates with odometer and other features.\n","\n","2. ***Price per mile*** I created a new feature :  Price per mile combining price and miles\n","\n","3. ***Manufacturer reduction of categories***: To reduce the number of categories in the manufacturer column I grouped the infrequent ones into an \"Other\" category.\n","\n","\n","\n"],"metadata":{"id":"-bbvxXOydViv"}},{"cell_type":"markdown","source":["1 . **CarAge**"],"metadata":{"id":"_6tbkZIWAfmL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woKPIV3pSrMb","executionInfo":{"status":"ok","timestamp":1738549580150,"user_tz":480,"elapsed":184,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"6905392e-375a-403d-b7b5-039d850e241d"},"outputs":[{"output_type":"stream","name":"stdout","text":["   year  age\n","0  2013   12\n","1  2013   12\n","2  2013   12\n","3  2013   12\n","4  2013   12\n"]}],"source":["# import pandas as pd\n","import datetime\n","\n","# Load dataset\n","# cars = pd.read_csv('data/vehicles.csv')\n","\n","# Assume 'year' contains the car's production year and convert it to integer if needed\n","cars['year'] = cars['year'].fillna(0).astype(int)\n","\n","# Get the current year\n","current_year = datetime.datetime.now().year\n","\n","# Compute the age of the car\n","cars['age'] = current_year - cars['year']\n","\n","# Check the result\n","print(cars[['year', 'age']].head())"]},{"cell_type":"markdown","source":["2. **Price per mile**"],"metadata":{"id":"SX-JhgHAi3kL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9PI5rJWSrMb"},"outputs":[],"source":["# missing values in the 'odometer' column by imputing with the median.\n","cars['odometer'].fillna(cars['odometer'].median(), inplace=True)\n","\n","# Avoid division by zero: Replace zero odometer values with NaN, then drop these rows\n","cars.loc[cars['odometer'] == 0, 'odometer'] = np.nan\n","cars = cars.dropna(subset=['odometer'])\n","\n","# Create the new feature: price per mile\n","cars['price_per_mile'] = cars['price'] / cars['odometer']\n","\n","# Display the first few rows to verify the new feature\n","print(cars[['price', 'odometer', 'price_per_mile']].head())"]},{"cell_type":"markdown","source":["3. **Manufacturer Category Reduction**"],"metadata":{"id":"EURD4EZkAurp"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","# cars = pd.read_csv('data/vehicles.csv')\n","\n","# Calculate the relative frequency of each manufacturer\n","manufacturer_freq = cars['manufacturer'].value_counts(normalize=True)\n","\n","# Define a threshold: keep manufacturers with at least 1% of the records\n","threshold = 0.01 # Intersting 0.05 only give me Toyota, Chevrolet and ford I reduced to 0.01\n","# Identify manufacturers that meet or exceed the threshold\n","frequent_manufacturers = manufacturer_freq[manufacturer_freq >= threshold].index\n","\n","# Create a new column where infrequent manufacturers are replaced with 'Other'\n","cars['manufacturer_reduced'] = cars['manufacturer'].apply(\n","    lambda x: x if x in frequent_manufacturers else 'Other'\n",")\n","\n","# Optionally, display the counts for the new column to verify the transformation\n","print(cars['manufacturer_reduced'].value_counts())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5gq1RR1k9UQ","executionInfo":{"status":"ok","timestamp":1738549617757,"user_tz":480,"elapsed":530,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"527cee2e-5d1d-44db-f934-1dc0df4b5843","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["manufacturer_reduced\n","ford             70497\n","chevrolet        54862\n","toyota           34131\n","Other            26121\n","honda            21206\n","nissan           19040\n","jeep             18975\n","ram              18160\n","Unknown          17341\n","gmc              16734\n","bmw              14692\n","dodge            13550\n","mercedes-benz    11801\n","hyundai          10300\n","subaru            9480\n","volkswagen        9324\n","kia               8440\n","lexus             8189\n","audi              7563\n","cadillac          6840\n","chrysler          6014\n","acura             5972\n","buick             5487\n","mazda             5407\n","infiniti          4789\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","source":["**Data Preparation - Answer4**\n","\n","1. **Encoding Categorical Variables** convert categorical variables into numerical representations.\n","2. **One-Hot Encoding:** For nominal categories with no intrinsic order\n","3. **Ordinal Encoding:** If there’s a natural order\n"],"metadata":{"id":"_n7zBQDsmL1P"}},{"cell_type":"markdown","source":["1. Encoding Categorical Variables"],"metadata":{"id":"Ed1jKOOZBafm"}},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Berkeley/Unit11/practical_application_II')\n","dtype_dict = {\n","    'price': 'int32',\n","    'year': 'float32',\n","    'odometer': 'float32'\n","}\n","cars = pd.read_csv('data/vehicles.csv', dtype=dtype_dict)\n","cars.drop(columns=['VIN'], inplace=True)\n","\n","# and take  a random 50% of the total rows of the dataset\n","cars_sample = cars.sample(frac=0.5, random_state=42)\n","\n","# Create dummy variables for selected categorical columns\n","cols_to_encode = ['manufacturer', 'cylinders', 'fuel',\n","                      'transmission']\n","\n","# categorical_cols = ['region', 'manufacturer', 'model', 'condition',\n","#                    'cylinders', 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state']\n","# cars_encoded = pd.get_dummies(cars_sample, columns=categorical_cols, drop_first=True)\n","\n","cars_encoded = pd.get_dummies(cars_sample, columns=cols_to_encode, drop_first=True)\n","\n","# Display the number of columns in the new DataFrame\n","num_new_columns = cars_encoded.shape[1]\n","print(f'The number of new columns created is: {num_new_columns}')"],"metadata":{"id":"MYyKXbolmd4f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738723128587,"user_tz":480,"elapsed":3662,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"9bda0bd7-33c1-4045-8863-84f8ab42e98a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","The number of new columns created is: 67\n"]}]},{"cell_type":"markdown","source":["2. One-Hot Encoding\n","Is done through pd.get_dummies() which creates a binary indicator or dummy for each category in the specified in columns=categorical_cols. I found that drop_first =True is a good practice to drop the first category to avoid multicollinearity when the encoded variables are used in linear regression. I learned that when I perform one-hot encoding, each categorical variable with K unique categories is transformed into K binary (dummy) variables. If I include all k\n","dummy variables along with an intercept in my linear regression model, there is an inherent linear dependency because for any observation the sum of these K dummy variables is always 1. This situation can lead to erroneous coefficient estimates.\n","The first attempt to one hot encode all the columns that looks having limited set of values, resulted in a crazy number of columns created 21199\n","So, I decided to reduce to the ones that as far I my knowledge goes, could affect the price. manufacturer, cylinders, transmision, and fuel reducing the number of columns to 67"],"metadata":{"id":"p3uRev3UBkzP"}},{"cell_type":"code","source":[],"metadata":{"id":"r7cwO2BkBn15"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Ordinal Encoding. To do Ordinal encoding I decided to look into each \" categorical_cols\"='region', 'manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission', 'VIN', 'drive', 'size', 'type', 'paint_color', 'state'] to find which columns have few unique values and if those values can be sorted numerically, then those values are good candidate to be ordinal and I will flag the column. After that I will inspect the column by myself to check if those are real ordinals. The following code try to do this exercise\n","After running this code I can conclude that columns:  condition, size, title_status, drive.  But, before to make the Ordinal Encoding I"],"metadata":{"id":"HqM_k9M4Bo04"}},{"cell_type":"code","source":["import pandas as pd\n","# This is my list of candidate columns having ordinal values\n","Potential_ordinal_cols = ['region', 'condition','title_status','drive', 'size', 'type', 'paint_color', 'state']\n","\n","def is_convertible_to_float(val):\n","    try:\n","        float(val)\n","        return True\n","    except (ValueError, TypeError):\n","        return False\n","\n","def is_ordinal_candidate(series, threshold_for_unique_values=10, numeric_ratio_threshold=0.8):\n","    \"\"\"\n","     checkin if the serie is a candidate for ordinal\n","\n","    Parameters:\n","      series (pd.Series): The categorical column belonging to Potential_ordinal_cols\n","      threshold_for_unique_values (int): If the number of unique values is less than or equal\n","                              to this, we consider it as a potential candidate.\n","      numeric_ratio_threshold (float): If most (>= this fraction) of the unique values\n","                                       are convertible to numeric, consider it ordinal.\n","\n","    Returns:\n","      bool: True if the series is a candidate for ordinal encoding, False otherwise.\n","    \"\"\"\n","    # Drop missing values and get unique values\n","    unique_vals = series.dropna().unique()\n","    n_unique = len(unique_vals)\n","\n","    # Heuristic 1: Very few unique values suggests a candidate.\n","    if n_unique <= threshold_for_unique_values:\n","        return True\n","\n","    # Heuristic 2: Check if most unique values can be converted to float.\n","    numeric_count = sum(is_convertible_to_float(val) for val in unique_vals)\n","    if (numeric_count / n_unique) >= numeric_ratio_threshold:\n","        return True\n","\n","    # Otherwise, not flagged as ordinal.\n","    return False\n","\n","# Loop through each categorical column and flag potential ordinal candidates.\n","for col in Potential_ordinal_cols:\n","    candidate = is_ordinal_candidate(cars[col])\n","    n_unique = cars[col].nunique(dropna=True)\n","    if candidate:\n","        print(f\"Column '{col}' (unique values: {n_unique}) ordinal encoding good candidate\")\n","    else:\n","        print(f\"Column '{col}' (unique values: {n_unique}) no ordinal.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738606968684,"user_tz":480,"elapsed":695,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"eab57537-353c-46ed-ff15-d677bb47c86e","id":"79TMi6iWDbNN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Column 'region' (unique values: 404) no ordinal.\n","Column 'condition' (unique values: 6) ordinal encoding good candidate\n","Column 'title_status' (unique values: 6) ordinal encoding good candidate\n","Column 'drive' (unique values: 3) ordinal encoding good candidate\n","Column 'size' (unique values: 4) ordinal encoding good candidate\n","Column 'type' (unique values: 13) no ordinal.\n","Column 'paint_color' (unique values: 12) no ordinal.\n","Column 'state' (unique values: 51) no ordinal.\n"]}]},{"cell_type":"markdown","source":["Before to do ordinal encoding to all candidates I want to see the real values to see if make sense or not\n"],"metadata":{"id":"9culgoDMEWOF"}},{"cell_type":"code","source":["columns_to_check = ['condition', 'title_status', 'drive', 'size']\n","\n","for col in columns_to_check:\n","    # Drop missing values, get unique values, and sort them\n","    unique_vals = sorted(cars[col].dropna().unique())\n","    print(f\"Unique values in '{col}' (alphabetically sorted):\")\n","    print(unique_vals)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWq4NxLRFBeS","executionInfo":{"status":"ok","timestamp":1738607452066,"user_tz":480,"elapsed":177,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"a302be8f-3a23-4035-c16e-70bed3e6de1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique values in 'condition' (alphabetically sorted):\n","['excellent', 'fair', 'good', 'like new', 'new', 'salvage']\n","\n","Unique values in 'title_status' (alphabetically sorted):\n","['clean', 'lien', 'missing', 'parts only', 'rebuilt', 'salvage']\n","\n","Unique values in 'drive' (alphabetically sorted):\n","['4wd', 'fwd', 'rwd']\n","\n","Unique values in 'size' (alphabetically sorted):\n","['compact', 'full-size', 'mid-size', 'sub-compact']\n","\n"]}]},{"cell_type":"markdown","source":["The ordinals I see are ***condition***  ***size***  and ***title_status***\n","\n","1. ***condition*** =  \"salvage\" < \"fair\" < \"good\" < \"excellent\" < \"like new\" < \"new\"  and\n","2. ***size*** = \"compact\" < \"midsize\" < \"fullsize\"\n","3. ***title_status***  = \"salvage\" < \"rebuilt\" < \"clean\"\n","\n","Now I can create a dictionary with the order of those ordinals to create ordinal encoded columns in the following code"],"metadata":{"id":"JLWj3JOiFpsN"}},{"cell_type":"code","source":["My_ordinal_mapping = {\n","    'condition': {\n","        'salvage': 1,\n","        'fair': 2,\n","        'good': 3,\n","        'excellent': 4,\n","        'like new': 5,\n","        'new': 6\n","    },\n","    'size': {\n","        'compact': 1,\n","        'midsize': 2,\n","        'fullsize': 3\n","    },\n","    'title_status': {\n","        'salvage': 1,\n","        'rebuilt': 2,\n","        'clean': 3\n","    }\n","}\n","\n","# Print the ordinal mapping for each specified column\n","for col, mapping in My_ordinal_mapping.items():\n","    print(f\"Ordinals for '{col}':\")\n","    for category, ordinal_value in mapping.items():\n","        print(f\"  {category}: {ordinal_value}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Udvbnx-MIEBi","executionInfo":{"status":"ok","timestamp":1738608388034,"user_tz":480,"elapsed":202,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"322cddb1-f294-4730-96d8-19f64c202351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ordinals for 'condition':\n","  salvage: 1\n","  fair: 2\n","  good: 3\n","  excellent: 4\n","  like new: 5\n","  new: 6\n","\n","Ordinals for 'size':\n","  compact: 1\n","  midsize: 2\n","  fullsize: 3\n","\n","Ordinals for 'title_status':\n","  salvage: 1\n","  rebuilt: 2\n","  clean: 3\n","\n"]}]},{"cell_type":"markdown","source":["Now I can create new columns with those ordinals:"],"metadata":{"id":"TOb9yMSzIuFf"}},{"cell_type":"code","source":["cars['condition_ordinal'] = cars['condition'].map(My_ordinal_mapping['condition'])\n","cars['size_ordinal'] = cars['size'].map(My_ordinal_mapping['size'])\n","cars['title_status_ordinal'] = cars['title_status'].map(My_ordinal_mapping['title_status'])"],"metadata":{"id":"D5SsyBUQI4XH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Preparation - Answer5**\n","\n","\n","1. Scaling and Normalization.  For machine learning algorithms based on distance metrics or regularization is recommended to use scale numerical features using StandardScaler from sklearn.preprocessing lib\n","\n","2. Pipeline Integration.After doing a lot of transformations, I need to create a pipeline to integrate all the preprocessing steps and then fits a model. This will ensures that all my transformations are applied consistently during cross-validation and on new data. very important ! :-)\n"],"metadata":{"id":"gtADYzh9oyvO"}},{"cell_type":"markdown","source":["1. Scaling & Normalization"],"metadata":{"id":"mS2My-8tDds7"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# Define numerical columns (including transformed features)\n","numerical_cols = ['price', 'year', 'odometer_log', 'age']\n","\n","# Build a ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_cols),\n","        # I should add a transformer for categorical variables here ( If I have time :-)\n","    ],\n","    remainder='passthrough'\n",")"],"metadata":{"id":"5FG4LtILpOwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Pipeline: All the transformation following the CRISP-DM methodology."],"metadata":{"id":"-qnrAW0yDGdT"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Berkeley/Unit11/practical_application_II')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ur-sy5ri5Fet","executionInfo":{"status":"ok","timestamp":1739837984175,"user_tz":480,"elapsed":30862,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"e69c0ffe-5f2e-4096-e966-0efe12eae854"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import datetime\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import FunctionTransformer, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","\n","\n","# Because I got Session Crashed after using all available RAM\n","# I decided to drop the column VIN ( I don't think the VIN number has any relevant importance predicting the price )\n","# and used dtype_dict\n","dtype_dict = {\n","    'price': 'int32',\n","    'year': 'float32',\n","    'odometer': 'float32'\n","}\n","cars = pd.read_csv('data/vehicles.csv', dtype=dtype_dict)\n","cars.drop(columns=['VIN'], inplace=True)"],"metadata":{"id":"E06boQtqPtqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import datetime\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import FunctionTransformer, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","\n","# Because I got Session Crashed after using all available RAM\n","# I decided to drop the column VIN ( I don't think the VIN number has any relevant importance predicting the price )\n","# and used dtype_dict\n","dtype_dict = {\n","    'price': 'int32',\n","    'year': 'float32',\n","    'odometer': 'float32'\n","}\n","cars = pd.read_csv('data/vehicles.csv', dtype=dtype_dict)\n","cars.drop(columns=['VIN'], inplace=True)\n","\n","# and take  a random 50% of the total rows of the dataset\n","cars_sample = cars.sample(frac=0.5, random_state=42)\n","\n","numerical_cols = ['price', 'year', 'odometer']  # Before log transformation\n","# Build a ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_cols),\n","        # I could add more and more transformers for categorical variables here as needed,but no time :-)\n","    ],\n","    remainder='passthrough'\n",")\n","\n","\n","# ========= Step 1: Outlier Detection and Removal =========\n","def remove_outliers_iqr(df, numeric_cols=['price', 'year', 'odometer'], factor=1.5):\n","    \"\"\"\n","    Removing those rows from the numeroc colums with outlier values using the IQR method.\n","\n","    Parameters:\n","      df (pd.DataFrame): Input DataFrame.\n","      numeric_cols (list): List of numeric columns to check.\n","      factor (float): Multiplier for the IQR to define outlier thresholds.\n","\n","    Returns:\n","      pd.DataFrame: DataFrame with outliers removed.\n","    \"\"\"\n","    df = df.copy()\n","    for col in numeric_cols:\n","        Q1 = df[col].quantile(0.25)\n","        Q3 = df[col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - factor * IQR\n","        upper_bound = Q3 + factor * IQR\n","        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n","    return df\n","\n","# ========= Step 2: Missing Value Imputation =========\n","def impute_missing_values(df):\n","    df = df.copy()\n","    # For numeric columns, fill missing values with the median\n","    numeric_cols = ['year', 'odometer', 'price']\n","    for col in numeric_cols:\n","        df[col] = df[col].fillna(df[col].median())\n","\n","    # For categorical columns, fill missing values with 'Unknown'\n","    # Not considering state .. I don't think the state could make any difference\n","    categorical_cols = ['region', 'manufacturer', 'model', 'condition',\n","                        'cylinders', 'fuel', 'title_status', 'transmission',\n","                        'drive', 'size', 'type', 'paint_color']\n","    for col in categorical_cols:\n","        df[col] = df[col].fillna('Unknown')\n","        df['year'] = df['year'].astype('int32')\n","\n","    return df\n","\n","# ========= Step 3: Outlier Treatment and Odometer Transformation =========\n","def process_odometer(df):\n","    df = df.copy()\n","    # Cap the odometer values at the 1st and 99th percentiles\n","    lower_bound = df['odometer'].quantile(0.01)\n","    upper_bound = df['odometer'].quantile(0.99)\n","    df['odometer_capped'] = df['odometer'].clip(lower=lower_bound, upper=upper_bound)\n","\n","    # Log-transform the capped odometer to reduce right skewness (use log1p to handle zero values)\n","    df['odometer_log'] = np.log1p(df['odometer_capped'])\n","    return df\n","\n","# ========= Step 4: Feature Engineering =========\n","def feature_engineering(df):\n","    df = df.copy()\n","    # Create a new feature: car age (current year - production year)\n","    current_year = datetime.datetime.now().year\n","    df['age'] = current_year - df['year'].astype(int)\n","\n","    # Create price per mile; avoid division by zero by replacing zero with NaN first\n","    # instead of remove the whole row to preserv potential valid data. Like new vehicles.\n","    # can introduce some bias in my analysis...\n","    # Create price per mile; handle potential division by zero by filling with NaN first\n","    df['price_per_mile'] = df['price'] / df['odometer']\n","    # Replace inf with NaN\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    # Fill NaN values with median of price_per_mile column\n","    df['price_per_mile'].fillna(df['price_per_mile'].median(), inplace=True)\n","\n","    return df\n","\n","# ========= Step 5: Reduce Categories for Manufacturer =========\n","def reduce_manufacturer(df, threshold=0.01):\n","  # I tested several values for the threshold starting with 0.05, I realized 0.01 gives me fair list of\n","  # manufacturers. O.05 only gaveme Toyota Chevrolet and Ford\n","    df = df.copy()\n","    # Calculate the relative frequency of each manufacturer\n","    freq = df['manufacturer'].value_counts(normalize=True)\n","    # Identify manufacturers that represent at least 'threshold' of observations\n","    frequent = freq[freq >= threshold].index\n","    # Create a new column where infrequent manufacturers are replaced with 'Other'\n","    df['manufacturer_reduced'] = df['manufacturer'].apply(\n","        lambda x: x if x in frequent else 'Other'\n","    )\n","    return df\n","\n","# ========= Step 6: Ordinal Encoding =========\n","def ordinal_encoding(df):\n","    df = df.copy()\n","    # Based in previous analysis These are the ordinal mappings..\n","    condition_mapping = {\n","        'salvage': 1,\n","        'fair': 2,\n","        'good': 3,\n","        'excellent': 4,\n","        'like new': 5,\n","        'new': 6,\n","        'Unknown': np.nan\n","    }\n","    title_status_mapping = {\n","        'salvage': 1,\n","        'rebuilt': 2,\n","        'clean': 3,\n","        'Unknown': np.nan\n","    }\n","    size_mapping = {\n","        'compact': 1,\n","        'midsize': 2,\n","        'fullsize': 3,\n","        'Unknown': np.nan\n","    }\n","\n","    df['condition_ordinal'] = df['condition'].map(condition_mapping)\n","    df['title_status_ordinal'] = df['title_status'].map(title_status_mapping)\n","    df['size_ordinal'] = df['size'].map(size_mapping)\n","\n","    return df\n","\n","# ========= Step 7: One-Hot Encoding for Remaining Categorical Variables =========\n","def one_hot_encoding(df):\n","    df = df.copy()\n","    # List the remaining categorical columns to one-hot encode.\n","    # Exclude columns that have already been ordinal-encoded or transformed.\n","    cols_to_encode = ['manufacturer', 'cylinders', 'fuel',\n","                      'transmission']\n","    df = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)\n","    return df\n","\n","# ========= Full Preprocessing Function =========\n","def full_preprocessing(df):\n","\n","    # Step 0: Apply ColumnTransformer for scaling (NEW STEP)\n","    df_transformed = preprocessor.fit_transform(df) # return a numpy array\n","    # ----> Convert back to DataFrame (important!)\n","    remainder_columns = [col for col in df.columns if col not in numerical_cols]\n","    all_columns = numerical_cols + remainder_columns\n","    df = pd.DataFrame(df_transformed, columns=all_columns)\n","\n","    df = pd.DataFrame(df, columns=numerical_cols + list(df.columns[len(numerical_cols):]))\n","    # Step 1: Remove outliers using the IQR method\n","    df = remove_outliers_iqr(df, numeric_cols = numerical_cols, factor=1.5)\n","    # Step 2: Impute missing values\n","    df = impute_missing_values(df)\n","    # Step 3: Process the odometer variable (cap and log-transform)\n","    df = process_odometer(df)\n","    # Step 4: Create additional features (age, price per mile)\n","    df = feature_engineering(df)\n","    # Step 5: Reduce manufacturer categories\n","    df = reduce_manufacturer(df)\n","    # Step 6: Apply ordinal encoding on select columns\n","    df = ordinal_encoding(df)\n","    # Step 7: One-hot encode the remaining categorical variables\n","    df = one_hot_encoding(df)\n","    return df\n","\n","# ========= Create the Pipeline =========\n","preprocessing_pipeline = Pipeline(steps=[\n","    ('full_preprocessing', FunctionTransformer(full_preprocessing))\n","])\n","\n","# ========= Apply the Pipeline =========\n","# need to load the sample 50% to avoid memory RAM exhausting\n","# cars = pd.read_csv('data/vehicles.csv')\n","\n","# Run the full preprocessing pipeline with the 50% of the full date set to avoid memory issues\n","cars_preprocessed = preprocessing_pipeline.transform(cars_sample)\n","\n","# Display the first few rows of the preprocessed data\n","print(cars_preprocessed.head())\n","\n"],"metadata":{"id":"6DgK2cwz5YGx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739850014891,"user_tz":480,"elapsed":5852,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"f1576bc4-719e-4119-9bf2-d8564ac22faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-939525ec2d46>:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[col] = df[col].fillna(df[col].median())\n","<ipython-input-16-939525ec2d46>:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[col] = df[col].fillna(df[col].median())\n","<ipython-input-16-939525ec2d46>:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[col] = df[col].fillna(df[col].median())\n","<ipython-input-16-939525ec2d46>:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","<ipython-input-16-939525ec2d46>:102: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['price_per_mile'].fillna(df['price_per_mile'].median(), inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":["      price  year  odometer          id                  region  \\\n","0  0.003609     0 -0.270731  7315883828                lakeland   \n","3 -0.000587     0 -0.024469  7312663807      northern panhandle   \n","4 -0.003020     0 -0.230290  7315368523                  eugene   \n","5 -0.002642     0  0.586199  7309863303  waterloo / cedar falls   \n","6 -0.000351     0 -0.277230  7315163492                 jackson   \n","\n","                   model  condition title_status    drive     size  ...  \\\n","0  f150 super cab lariat       good        clean      4wd  Unknown  ...   \n","3                   328i    Unknown        clean  Unknown  Unknown  ...   \n","4            suburban ls    Unknown        clean  Unknown  Unknown  ...   \n","5           town country  excellent        clean      fwd  Unknown  ...   \n","6        outlander sport  excellent        clean      fwd  Unknown  ...   \n","\n","  cylinders_Unknown cylinders_other fuel_diesel  fuel_electric  fuel_gas  \\\n","0             False           False       False          False      True   \n","3              True           False       False          False      True   \n","4             False           False       False          False     False   \n","5             False           False       False          False      True   \n","6             False           False       False          False      True   \n","\n","   fuel_hybrid  fuel_other transmission_automatic  transmission_manual  \\\n","0        False       False                  False                False   \n","3        False       False                   True                False   \n","4        False        True                   True                False   \n","5        False       False                   True                False   \n","6        False       False                   True                False   \n","\n","   transmission_other  \n","0                True  \n","3               False  \n","4               False  \n","5               False  \n","6               False  \n","\n","[5 rows x 78 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Now I need to create training data set and test dataset\n","# Separate features (X) and target (y)\n","X = cars_preprocessed.drop(\"price\", axis=1)\n","y = cars_preprocessed[\"price\"]\n","\n","# Split the data: 80% training, 20% testing (adjust test_size as needed)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Optionally, print the shapes to verify the split\n","print(\"Training set shape:\", X_train.shape, y_train.shape)\n","print(\"Testing set shape:\", X_test.shape, y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IWJa-zXT_U8","executionInfo":{"status":"ok","timestamp":1739839167637,"user_tz":480,"elapsed":152,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"b251c638-d108-4c0e-a584-2751b29c8f65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape: (157738, 77) (157738,)\n","Testing set shape: (39435, 77) (39435,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"g_h3-HEASrMb"},"source":["### Modeling\n","\n","With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."]},{"cell_type":"markdown","source":["Linear Regression\n","No hyperparameters\n","I'm going to use it as a base line"],"metadata":{"id":"W5jiMprIVNaZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqfmYREiSrMb","executionInfo":{"status":"ok","timestamp":1738732207115,"user_tz":480,"elapsed":315,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"923b4b5b-b7a9-4338-a317-ee0724ab4ccf"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-2c31a22ab4fd>:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  X_train[feature].fillna(X_train[feature].median(), inplace=True)\n","<ipython-input-15-2c31a22ab4fd>:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  X_test[feature].fillna(X_train[feature].median(), inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Model 1 using features: ['year', 'odometer_log', 'age', 'price_per_mile']\n","  Mean Squared Error: 129011559.82\n","  Mean Absolute Error: 8729.75\n","\n","Model 2 using features: ['year', 'odometer_log', 'age', 'price_per_mile', 'condition_ordinal', 'title_status_ordinal', 'size_ordinal']\n","  Mean Squared Error: 127873911.88\n","  Mean Absolute Error: 8658.76\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.impute import SimpleImputer\n","\n","# Model 1: Simple model\n","model1_features = ['year', 'odometer_log', 'age', 'price_per_mile']\n","\n","#Create SimpleImputer instance: Created an instance of SimpleImputer with strategy='median'.\n","# This will replace NaN values with the median of the respective column, I could use main as well\n","imputer = SimpleImputer(strategy='median')\n","\n","# Model 2: Extended model with the ordinal encoded columns.\n","model2_features = model1_features + ['condition_ordinal', 'title_status_ordinal', 'size_ordinal']\n","\n","# Model 3: Full model using all features (all columns in X_train)\n","model3_features = X_train.columns.tolist()\n","\n","# --- Build, fit, and evaluate the models ---\n","\n","MyResults = {}  # to store evaluation metrics for each model\n","\n","# Model 1\n","model1 = LinearRegression()\n","\n","# Impute NaN values in X_train and X_test with the median (or another strategy) before fitting\n","# For each column in model1_features, fill NaN with the median of that column in X_train\n","for feature in model1_features:\n","    X_train[feature].fillna(X_train[feature].median(), inplace=True)\n","    X_test[feature].fillna(X_train[feature].median(), inplace=True)\n","\n","model1.fit(X_train[model1_features], y_train)\n","pred1 = model1.predict(X_test[model1_features])\n","mse1 = mean_squared_error(y_test, pred1)\n","mae1 = mean_absolute_error(y_test, pred1)\n","MyResults['Model 1'] = {'Features': model1_features, 'MSE': mse1, 'MAE': mae1}\n","\n","# Model 2\n","model2 = LinearRegression()\n","# use the imputer\n","X_train_model2 = pd.DataFrame(imputer.fit_transform(X_train[model2_features]), columns=model2_features, index=X_train.index)\n","X_test_model2 = pd.DataFrame(imputer.transform(X_test[model2_features]), columns=model2_features, index=X_test.index)\n","\n","model2.fit(X_train_model2, y_train)\n","pred2 = model2.predict(X_test_model2)\n","\n","mse2 = mean_squared_error(y_test, pred2)\n","mae2 = mean_absolute_error(y_test, pred2)\n","MyResults['Model 2'] = {'Features': model2_features, 'MSE': mse2, 'MAE': mae2}\n","\n","# Model 3  -------------------------------------------------------------------------------------\n","# the full linear regression give me errors ValueError: Cannot use median strategy with non-numeric data: could not convert string to float:\n","# need more time to debug\n","# model3 = LinearRegression()\n","#I got this error The error \"ValueError: Cannot use median strategy with non-numeric data: could not convert string to float:\n","#I have to identify numerocal features in  X_train[model3_features] using select dtypes(include=np.number) saving in numerical_features_model\n","# Select only numerical features for imputation\n","# numerical_features_model3 = X_train[model3_features].select_dtypes(include=np.number).columns.tolist()\n","\n","# Apply imputation only to numerical features\n","# X_train_model3_num = pd.DataFrame(imputer.fit_transform(X_train[numerical_features_model3]),\n","#                                  columns=numerical_features_model3, index=X_train.index)\n","#X_test_model3_num = pd.DataFrame(imputer.transform(X_test[numerical_features_model3]),\n","#                                 columns=numerical_features_model3, index=X_test.index)\n","\n","# X_train_model3 = pd.concat([X_train_model3_num, X_train[model3_features].select_dtypes(exclude=np.number)], axis=1)\n","# X_test_model3 = pd.concat([X_test_model3_num, X_test[model3_features].select_dtypes(exclude=np.number)], axis=1)\n","# Previous code with error : X_train_model3 = pd.DataFrame(imputer.fit_transform(X_train[model3_features]), columns=model3_features, index=X_train.index)\n","# Previous code with error : X_test_model3 = pd.DataFrame(imputer.transform(X_test[model3_features]), columns=model3_features, index=X_test.index)\n","# model3.fit(X_train_model3, y_train)\n","# pred3 = model3.predict(X_test_model3)\n","# mse3 = mean_squared_error(y_test, pred3)\n","# mae3 = mean_absolute_error(y_test, pred3)\n","# MyResults['Model 3'] = {'Features': model3_features, 'MSE': mse3, 'MAE': mae3}\n","\n","# --- Print the evaluation metrics for each model ---\n","for model_name, res in MyResults.items():\n","    print(f\"\\n{model_name} using features: {res['Features']}\")\n","    print(f\"  Mean Squared Error: {res['MSE']:.2f}\")\n","    print(f\"  Mean Absolute Error: {res['MAE']:.2f}\")\n","\n","### --->>> NEED TO APPLY SEQUENTIAL FEATURE SELECTION w9.2 w9.3 w9.4 Rdge Model <<<< -------------\n","\n","### ->>. GridSearchCV Best Alphaiterating over alphas\n"]},{"cell_type":"markdown","source":["2 Ridge Regression\n","Parameter = alpha  but the range should be in log scale 10^-3 to 10^3  "],"metadata":{"id":"lVKvHyoBVjm5"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.stats import uniform\n","\n","features_Ridge = ['year', 'odometer_log', 'age', 'price_per_mile']\n","# Option 1 using GridSearchCV to tune it\n","# Define Ridge estimator with a random state equal 42 to repro acn compare\n","# plan to tuse auto and svd as the algorithms to solve the best coef,  I believe we\n","# have some correlation between variables in the dataset.\n","MyRidge = Ridge(random_state=42)\n","MyGridSearchCV_params = {\n","    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver':['auto', 'svd'],\n","    'max_iter':[None,100,1000]\n","}\n","# Set up GridSearchCV with 5-fold cross-validation and negative MSE as the scoring metric\n","grid_search = GridSearchCV(\n","    estimator= MyRidge,\n","    param_grid= MyGridSearchCV_params,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1\n",")\n","grid_search.fit(X_train[features_Ridge], y_train)\n","\n","# Retrieve the best Ridge estimator found by GridSearchCV\n","best_ridge_grid = grid_search.best_estimator_\n","print(\"Best parameters from GridSearchCV:\", grid_search.best_params_)\n","\n","# Evaluate the best estimator on the test set\n","prediction_grid = best_ridge_grid.predict(X_test[features_Ridge])\n","\n","mse_grid = mean_squared_error(y_test, prediction_grid)\n","mae_grid = mean_absolute_error(y_test, prediction_grid)\n","print(\"GridSearchCV Ridge - Test MSE:\", mse_grid)\n","print(\"GridSearchCV Ridge - Test MAE:\", mae_grid)\n","\n","\n","# Using RandomizeSearchCV to compare with GridSearchCV\n","# let's see :-)  using the same params\n","GridSearchCV_params = {\n","    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver':['auto', 'svd'],\n","    'max_iter':[None,100,1000]\n","}\n","MyRandom_search = RandomizedSearchCV(\n","    estimator= MyRidge,\n","    param_distributions= GridSearchCV_params,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1,\n","    n_iter=10,\n","    random_state=42\n",")\n","\n","MyRandom_search.fit(X_train[features_Ridge], y_train)\n","best_ridge_random = MyRandom_search.best_estimator_\n","print(\"Best parameters from RandomizedSearchCV:\", MyRandom_search.best_params_)\n","\n","prediction_MyRandom_search = best_ridge_random.predict(X_test[features_Ridge])\n","print(\"MyRandomizedSearchCV Ridge -Test MSE:\",mean_squared_error(y_test, prediction_MyRandom_search))\n","print(\"MyRandomizedSearchCV Ridge -Test MAE:\",mean_absolute_error(y_test, prediction_MyRandom_search))\n","\n","\n","\n"],"metadata":{"id":"lOKAStmKVhBq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739842973521,"user_tz":480,"elapsed":9307,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"483f64a3-a952-48c8-a98b-b81c3711204e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters from GridSearchCV: {'alpha': 1, 'max_iter': None, 'solver': 'auto'}\n","GridSearchCV Ridge - Test MSE: 4.464198509715481e-06\n","GridSearchCV Ridge - Test MAE: 0.0016075500395790945\n","Best parameters from RandomizedSearchCV: {'solver': 'svd', 'max_iter': 100, 'alpha': 1}\n","MyRandomizedSearchCV Ridge -Test MSE: 4.4641985097154785e-06\n","MyRandomizedSearchCV Ridge -Test MAE: 0.0016075500395791073\n"]}]},{"cell_type":"markdown","source":["3 Lasson Regression"],"metadata":{"id":"yEsDeiNkWP7E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEuSYLp4SrMc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843969305,"user_tz":480,"elapsed":15436,"user":{"displayName":"Paul Rivera","userId":"16279685022766326275"}},"outputId":"c39cc352-fe80-4f0c-813d-92e506a866a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters from GridSearchCV (Lasso): {'alpha': 0.001, 'max_iter': 1000, 'selection': 'cyclic', 'tol': 0.0001}\n","GridSearchCV Lasso - Test MSE: 5.7222877858253145e-06\n","GridSearchCV Lasso - Test MAE: 0.001990068505051068\n","Best parameters from RandomizedSearchCV (Lasso): {'alpha': 37.455011884736244, 'max_iter': 1000, 'selection': 'cyclic', 'tol': 0.01}\n","RandomizedSearchCV Lasso - Test MSE: 5.7222877858253145e-06\n","RandomizedSearchCV Lasso - Test MAE: 0.001990068505051068\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import Lasso\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.stats import uniform\n","\n","# Define the feature set (using the same features as for Ridge)\n","MyFeatures_Lasso = ['year', 'odometer_log', 'age', 'price_per_mile']\n","\n","# ---------------------------\n","# Option 1: Using GridSearchCV for Lasso\n","\n","# Initialize Lasso estimator with a fixed random_state for reproducibility\n","my_lasso = Lasso(random_state=42)\n","\n","# Define the parameter grid for GridSearchCV.\n","# Note: Lasso does not have a 'solver' parameter like Ridge, but it offers 'selection' (cyclic or random)\n","param_grid_lasso = {\n","    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'max_iter': [1000, 5000],\n","    'tol': [0.0001, 0.001, 0.01],\n","    'selection': ['cyclic', 'random']\n","}\n","\n","# Set up GridSearchCV with 5-fold cross-validation\n","lasso_grid_search = GridSearchCV(\n","    estimator=my_lasso,\n","    param_grid=param_grid_lasso,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1\n",")\n","\n","# Fit GridSearchCV on the training data\n","lasso_grid_search.fit(X_train[MyFeatures_Lasso], y_train)\n","\n","# Retrieve the best Lasso estimator found\n","best_lasso_grid = lasso_grid_search.best_estimator_\n","print(\"Best parameters from GridSearchCV (Lasso):\", lasso_grid_search.best_params_)\n","\n","# Evaluate the best estimator on the test set\n","pred_lasso_grid = best_lasso_grid.predict(X_test[MyFeatures_Lasso])\n","mse_lasso_grid = mean_squared_error(y_test, pred_lasso_grid)\n","mae_lasso_grid = mean_absolute_error(y_test, pred_lasso_grid)\n","print(\"GridSearchCV Lasso - Test MSE:\", mse_lasso_grid)\n","print(\"GridSearchCV Lasso - Test MAE:\", mae_lasso_grid)\n","\n","# ---------------------------\n","# Option 2: Using RandomizedSearchCV for Lasso\n","\n","# Define parameter distributions for RandomizedSearchCV.\n","# For 'alpha', we use a uniform distribution.  Let's see how it works .. :-)\n","param_dist_lasso = {\n","    'alpha': uniform(0.001, 100),\n","    'max_iter': [1000, 5000],\n","    'tol': [0.0001, 0.001, 0.01],\n","    'selection': ['cyclic', 'random']\n","}\n","\n","lasso_random_search = RandomizedSearchCV(\n","    estimator=my_lasso,\n","    param_distributions=param_dist_lasso,\n","    scoring='neg_mean_squared_error',\n","    cv=5,\n","    n_jobs=-1,\n","    n_iter=10,  # reduce iterations to speed up the search\n","    random_state=42\n",")\n","\n","# Fit RandomizedSearchCV on the training data\n","lasso_random_search.fit(X_train[MyFeatures_Lasso], y_train)\n","\n","# Retrieve the best Lasso estimator found\n","best_lasso_random = lasso_random_search.best_estimator_\n","print(\"Best parameters from RandomizedSearchCV (Lasso):\", lasso_random_search.best_params_)\n","\n","# Evaluate the best estimator on the test set\n","pred_lasso_random = best_lasso_random.predict(X_test[MyFeatures_Lasso])\n","mse_lasso_random = mean_squared_error(y_test, pred_lasso_random)\n","mae_lasso_random = mean_absolute_error(y_test, pred_lasso_random)\n","print(\"RandomizedSearchCV Lasso - Test MSE:\", mse_lasso_random)\n","print(\"RandomizedSearchCV Lasso - Test MAE:\", mae_lasso_random)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"oMK8jvWdyjXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NbQkWRSSrMc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SK1pgtrSrMc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"woONt4XjSrMc"},"source":["### Evaluation\n","\n","With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."]},{"cell_type":"markdown","source":["#### Evaluation Answer 1\n","\n","Both models perform very similar, but the Ridge model shows a slight advantage based on error metrics:\n","\n","Ridge Regression:\n","Test MSE: ~4.464e-06\n","Test MAE: ~0.001608\n","Lasso Regression:\n","Test MSE: ~5.722e-06\n","Test MAE: ~0.001990\n","\n"],"metadata":{"id":"av_CoXI2zoLt"}},{"cell_type":"markdown","source":["#### Evaluation Answer 2####\n","\n","1. Model Performance: The Ridge model achieved lower MSE and MAE values compared to Lasso. Maybe Ridge might be better at capturing the underlying relationships without introducing too much bias with a acceptable speed\n","\n","2. Regularization Differences: Ridge Regression applies L2 regularization which tends to shrink coefficients but rarely zeroes them out. Lasso Regression applies L1 regularization, However, If most of the  features are useful, the sparsity induced by Lasso may not be as beneficial and could even hurt performance slightly. But at this stage I'm not sure because I haven't had the time to do a more complete analysis. I think that is something I should do.\n","\n","3. Hyperparameter:\n","Both GridSearchCV and RandomizedSearchCV converged to similar performance metrics within each model, suggesting that the hyperparameter search was good.\n","For Ridge, both searches pointed to alpha = 1\n","For Lasso, although the best parameters differ between GridSearchCV and RandomizedSearchCV, the performance remained the same, indicating that several hyperparameter combinations may yield similar outcomes.\n","\n","4. Choosing the Best Model\n","Given the slightly lower error metrics, Ridge Regression looks like the best for this tasks. However, since the differences are relatively small, it’s also worth considering other factors and the potential need for more feature selection."],"metadata":{"id":"R3PtUYb4z6LY"}},{"cell_type":"markdown","source":["#### Evaluation Answer 3 ####\n","\n","What earlier adjustments can be done in the Data Preparation could be adjusted to get better results\n","\n","1. Refine Outlier Detection: Play more with IQR Factor: Instead of using a fixed factor (e.g., 1.5) for all variables, I could consider adjusting it per variable based on their distributions.\n","\n","2. Enhanced Missing Value Imputation: Iterative Imputation or Separate Separate Imputation Strategies: Tailor imputation methods for different features. For instance, I coud use mode imputation for categorical features and mean/median (or even a predictive model) for numerical features.\n","\n","3. Feature Transformation and Scaling: Target and Feature Transformation:\n","Apply logarithmic transformations to more variables (e.g., price) to stabilize variance and make the distributions more normal I only use log transformation to odometer\n","\n","4. More Correlation Analysis.  With more time I could examine multicollinearidad between predictors to remove or combine them in new features\n","\n"],"metadata":{"id":"XP1kGldL2USK"}},{"cell_type":"markdown","metadata":{"id":"Ww1JQJqeSrMc"},"source":["### Deployment\n","\n","Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."]},{"cell_type":"markdown","source":["### Used Car Sales Optimization : A Data-Driven Approach###\n","\n","**Introduction**\n","\n","In today’s competitive market, understanding what drives used car prices is crucial to managing your inventory effectively. My recent analysis leverages data science techniques to help you optimize your vehicle selection and pricing strategy. By identifying the key factors that influence car values, you can make better purchasing decisions, adjust pricing appropriately, and ultimately improve your profit margins.\n","\n","**What I analyzed**\n","\n","I examined a large dataset containing detailed information on hundreds of thousands of used (and new) cars. Key variables in my analysis included:\n","\n","Year: The production year of the vehicle, which helps determine its age.\n","Mileage (Odometer): Total miles driven, indicating wear and tear.\n","Price per Mile: A derived metric that divides the car’s price by its mileage, offering insight into relative value.\n","Other Factors: Additional features such as the vehicle's condition and manufacturer were also considered during the analysis.\n","\n","**The Modeling Process**\n","\n","To predict used car prices accurately, I applied two types of regression models:\n","Ridge Regression: This model applies a technique called L2 regularization, which shrinks the impact of each variable in a balanced way. My results showed that Ridge Regression performed very well, suggesting that all features in your dataset contribute valuable information to the final price.\n","Lasso Regression: Lasso uses L1 regularization, which can effectively “turn off” less important features. However, my analysis found that while Lasso was also effective, Ridge Regression slightly outperformed it, indicating that none of the features were completely redundant in predicting car prices.\n","\n","**Key Findings from the Models**\n","\n","**Ridge Regression Results:**\n","\n","Mean Squared Error (MSE): 4.46e-06\n","Mean Absolute Error (MAE): 0.00161\n","\n","**Lasso Regression Results:**\n","MSE: 5.72e-06\n","MAE: 0.00199\n","\n","The lower error rates in Ridge Regression mean that, on average, its predictions are closer to the actual sale prices. This suggests that when all factors are considered, each one plays a role in determining the car’s value.\n","\n","\n","**How This Helps You Fine-Tune your Inventory**\n","\n","1. Accurate Pricing Based on Key Features\n","Vehicle Age: Newer models tend to hold their value better. Use the car’s production year and calculated age to adjust pricing.\n","Mileage and Price per Mile: Lower mileage often indicates a higher value per mile. By comparing similar vehicles, you can identify bargains or overpriced listings.\n","2. Data-Driven Purchasing Decisions\n","Focus on Well-Rounded Vehicles: Since Ridge Regression shows that all measured features contribute to the final price, consider a holistic approach when evaluating potential purchases. Vehicles that score well across multiple factors are likely to be strong investments.\n","3. Inventory and Pricing Adjustments\n","Dynamic Pricing: Use these insights to set competitive prices that reflect the true market value, reducing the time vehicles spend on your lot.\n","Targeted Inventory Acquisition: Knowing which factors have the greatest impact on price can help you identify underpriced vehicles with potential for profit after refurbishment or market correction.\n","4. Continuous Improvement\n","Refining Data Collection: The more accurate and detailed your data (e.g., precise mileage, detailed condition reports), the more reliable your pricing models will be.\n","Regular Reassessment: The market is always evolving. Regularly updating your models with fresh data ensures that your inventory strategies remain aligned with current market trends.\n","Conclusion\n","Using data science, you can gain a competitive edge in the used car market. This analysis shows that a comprehensive approach—taking into account vehicle age, mileage, and other key features—provides a more accurate picture of a car’s value. In my study, Ridge Regression emerged as the more reliable tool for predicting prices, which suggests that a balanced consideration of all available features is beneficial.\n","\n","###My Recommendations###\n","\n","####Long term####\n","\n","Adopt data-driven pricing: Adjust your pricing strategy based on the factors that truly impact a car’s value.\n","Enhance your data collection: Gather detailed and accurate information on each vehicle.\n","Continuously review and update models: As market conditions change, so should your models and strategies.\n","\n","\n","\n","####Short Term####\n","\n","1. **Daily in Store  Inventory Assessment**  Morning Routine:\n","Run the Model on Current Inventory:\n","What to Do: Every morning, update your inventory data with the latest details (e.g., production year, mileage, condition).\n","How to Do It: Use the Python script you’ve built to feed your current inventory through the Ridge model. This model will output a predicted price for each vehicle.\n","Compare Predictions to Listed Prices:\n","What to Look For: Create a report that shows both the predicted price and the current listing price.\n","How to Act:\n","Overpriced Vehicles: If a car’s listing price is significantly above the predicted price, consider lowering the price or running a special promotion.\n","Underpriced Vehicles: If the listing price is lower than the predicted price, you might have room to increase the price—or if it’s selling too quickly, consider buying more of that model.\n","\n","2. **Pricing Adjustments**\n","In-Store and Online:\n","Adjust Listing Prices:\n","Action Step: Based on the morning report, update your online listings and in-store price tags.\n","Tip: Create a pricing band (e.g., within ±5% of the predicted price) as a target range for consistency.\n","Promotional Strategies:\n","Action Step: For vehicles that are overpriced compared to the model, consider running flash sales or discounts to move inventory faster.\n","Tip: Use the model’s confidence in its predictions as a guide—if the prediction error is low, you can be more aggressive with pricing changes.\n","\n","3. **Inventory Optimization**\n","Ongoing Inventory Review:\n","Identify High-Potential Vehicles:\n","Action Step: Use your model to flag vehicles that show a high “value per mile” or have lower mileage with a good production year.\n","How to Act: Prioritize these vehicles in marketing efforts or consider expanding your stock of similar models.\n","Evaluate Underperformers:\n","Action Step: For vehicles with a low predicted price relative to market demand, plan targeted promotions (e.g., “quick sale” discounts).\n","Tip: Keep a record of these vehicles to review if repeated adjustments are needed or if they should be phased out in future orders.\n","\n","4. **Weekly/Monthly  Analysis and Continuous Improvement**\n","End-of-Week and End-of-Month Review:\n","Analyze Sales Performance:\n","Action Step: At the end of each week, compare the actual sale prices with the model’s predictions.\n","How to Use: Identify trends—are vehicles sold closer to the predicted price? Are there any particular changes leading to faster sales? Enrich the model with those features\n","Adjust Procurement Decisions:\n","Action Step: Based on weekly/monthly performance, refine your buying strategy:\n","Increase orders for high-performing models.\n","Reduce or discount models that consistently underperform.\n","Feedback Loop:\n","Action Step: Update your models with the latest sales data. This helps improve prediction accuracy over time.\n","5. **Training and Maintenance**\n","Keep Your Team Trained and Informed:\n","Ongoing Training:\n","Action Step: Schedule periodic training sessions on how to interpret the model outputs and dashboard metrics.\n","How to Act: Share best practices and recent insights from your weekly reviews to empower your team in making data-driven decisions.\n","Tool Updates:\n","Action Step: Maintain and update your data pipeline. Ensure that new inventory data is incorporated in real time and that your models are re-trained as market conditions evolve\n","Keep adding new features as you discover they could affect the sales.\n","Call Data scientist to update the model  \n"],"metadata":{"id":"r9Xg3aSOPOrR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AggDm79KSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yT-qzQSGSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqBl5E5dSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NE1OSUn9SrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn_IOiyWSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TZZ9JcVSrMd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBY9vTbBSrMd"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[{"file_id":"1L5jQkraVEGX4Rqg8-CidH6lDCk6PXetX","timestamp":1738636979539}]}},"nbformat":4,"nbformat_minor":0}